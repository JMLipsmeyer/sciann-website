{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SciANN: A Keras wrapper for scientific computations and physics-informed deep learning using artificial neural networks You have just found SciANN. SciANN is a high-level artificial neural networks API, written in Python using Keras and TensorFlow backends. It is developed with a focus on enabling fast experimentation with different networks architectures and with emphasis on scientific computations, physics informed deep learing, and inversion. Being able to start deep-learning in a very few lines of code is key to doing good research. Use SciANN if you need a deep learning library that: Allows for easy and fast prototyping. Allows the use of complex deep neural networks. Takes advantage TensorFlow and Keras features including seamlessly running on CPU and GPU. Read the documentation at SicANN.com . Cite SciANN in your publications if it helps your research: @misc{haghighat2019sciann, title={SciANN: A Keras wrapper for scientific computations and physics-informed deep learning using artificial neural networks}, author={Haghighat, Ehsan and Juanes, Ruben}, howpublished={\\url{https://sciann.com}}, url = {https://github.com/sciann/sciann.git} year={2019} } SciANN is compatible with: Python 2.7-3.6 . Getting started: 30 seconds to SciANN The core data structure of SciANN is a Functional , a way to organize inputs ( Variables ) and outputs ( Fields ) of a network. Targets are imposed on Functional instances using Constraint s. The SciANN model ( SciModel ) is formed from inputs ( Variables ) and targets( Constraints ). The model is then trained by calling the solve function. Here is the simplest SciANN model: from sciann import Variable, Functional, SciModel from sciann.constraints import Data x = Variable('x') y = Functional('y') # y_true is a Numpy array of (N,1) -- with N as number of samples. model = SciModel(x, Data(y, y_true)) This is associated to the simplest neural network possible, i.e. a linear relation between the input variable x and the output variable y with only two parameters to be learned. Plotting a network is as easy as passing a file_name to the SciModel: model = SciModel(x, Data(y, y_true), plot_to_file='file_path') Once your model looks good, perform the learning with .solve() : # x_true is a Numpy array of (N,1) -- with N as number of samples. model.solve(x_true, epochs=5, batch_size=32) You can iterate on your training data in batches and in multiple epochs. Please check Keras documentation on model.fit for more information on possible options. You can evaluate the model any time on new data: classes = model.predict(x_test, batch_size=128) In the examples folder of the repository, you will find some examples of Linear Elasticity, Flow, Flow in Porous Media, etc. Installation Before installing Keras, you need to install the TensorFlow and Keras. TensorFlow installation instructions . Keras installation instructions . You may also consider installing the following optional dependencies : cuDNN (recommended if you plan on running Keras on GPU). HDF5 and h5py (required if you plan on saving Keras/SciANN models to disk). graphviz and pydot (used by visualization utilities to plot model graphs). Then, you can install SciANN itself. There are two ways to install SciANN: Install SciANN from PyPI (recommended): Note: These installation steps assume that you are on a Linux or Mac environment. If you are on Windows, you will need to remove sudo to run the commands below. sudo pip install sciann If you are using a virtualenv, you may want to avoid using sudo: pip install sciann Alternatively: install SciANN from the GitHub source: First, clone SciANN using git : git clone https://github.com/sciann/sciann.git Then, cd to the SciANN folder and run the install command: sudo python setup.py install or sudo pip install . Why this name, SciANN? Scientific Computational with Artificial Neural Networks. Scientific computations include solving ODEs, PDEs, Integration, Differentitation, Curve Fitting, etc.","title":"Home"},{"location":"#sciann-a-keras-wrapper-for-scientific-computations-and-physics-informed-deep-learning-using-artificial-neural-networks","text":"","title":"SciANN: A Keras wrapper for scientific computations and physics-informed deep learning using artificial neural networks"},{"location":"#you-have-just-found-sciann","text":"SciANN is a high-level artificial neural networks API, written in Python using Keras and TensorFlow backends. It is developed with a focus on enabling fast experimentation with different networks architectures and with emphasis on scientific computations, physics informed deep learing, and inversion. Being able to start deep-learning in a very few lines of code is key to doing good research. Use SciANN if you need a deep learning library that: Allows for easy and fast prototyping. Allows the use of complex deep neural networks. Takes advantage TensorFlow and Keras features including seamlessly running on CPU and GPU. Read the documentation at SicANN.com . Cite SciANN in your publications if it helps your research: @misc{haghighat2019sciann, title={SciANN: A Keras wrapper for scientific computations and physics-informed deep learning using artificial neural networks}, author={Haghighat, Ehsan and Juanes, Ruben}, howpublished={\\url{https://sciann.com}}, url = {https://github.com/sciann/sciann.git} year={2019} } SciANN is compatible with: Python 2.7-3.6 .","title":"You have just found SciANN."},{"location":"#getting-started-30-seconds-to-sciann","text":"The core data structure of SciANN is a Functional , a way to organize inputs ( Variables ) and outputs ( Fields ) of a network. Targets are imposed on Functional instances using Constraint s. The SciANN model ( SciModel ) is formed from inputs ( Variables ) and targets( Constraints ). The model is then trained by calling the solve function. Here is the simplest SciANN model: from sciann import Variable, Functional, SciModel from sciann.constraints import Data x = Variable('x') y = Functional('y') # y_true is a Numpy array of (N,1) -- with N as number of samples. model = SciModel(x, Data(y, y_true)) This is associated to the simplest neural network possible, i.e. a linear relation between the input variable x and the output variable y with only two parameters to be learned. Plotting a network is as easy as passing a file_name to the SciModel: model = SciModel(x, Data(y, y_true), plot_to_file='file_path') Once your model looks good, perform the learning with .solve() : # x_true is a Numpy array of (N,1) -- with N as number of samples. model.solve(x_true, epochs=5, batch_size=32) You can iterate on your training data in batches and in multiple epochs. Please check Keras documentation on model.fit for more information on possible options. You can evaluate the model any time on new data: classes = model.predict(x_test, batch_size=128) In the examples folder of the repository, you will find some examples of Linear Elasticity, Flow, Flow in Porous Media, etc.","title":"Getting started: 30 seconds to SciANN"},{"location":"#installation","text":"Before installing Keras, you need to install the TensorFlow and Keras. TensorFlow installation instructions . Keras installation instructions . You may also consider installing the following optional dependencies : cuDNN (recommended if you plan on running Keras on GPU). HDF5 and h5py (required if you plan on saving Keras/SciANN models to disk). graphviz and pydot (used by visualization utilities to plot model graphs). Then, you can install SciANN itself. There are two ways to install SciANN: Install SciANN from PyPI (recommended): Note: These installation steps assume that you are on a Linux or Mac environment. If you are on Windows, you will need to remove sudo to run the commands below. sudo pip install sciann If you are using a virtualenv, you may want to avoid using sudo: pip install sciann Alternatively: install SciANN from the GitHub source: First, clone SciANN using git : git clone https://github.com/sciann/sciann.git Then, cd to the SciANN folder and run the install command: sudo python setup.py install or sudo pip install .","title":"Installation"},{"location":"#why-this-name-sciann","text":"Scientific Computational with Artificial Neural Networks. Scientific computations include solving ODEs, PDEs, Integration, Differentitation, Curve Fitting, etc.","title":"Why this name, SciANN?"},{"location":"constraints/","text":"Constraints: [source] Data sciann.constraints.data.Data(cond, y_true=None, x_true_ids=None, name='data') Data class to impose to the system. Arguments cond (Functional): The Functional object that Data condition will be imposed on. y_true (np.ndarray): Expected output to set the pde to. If not provided, will be set to zero . x_true_ids (np.ndarray): A 1D numpy arrays consists of node-ids to impose the condition. name (String): A str for name of the pde. Returns Raises ValueError : 'cond' should be a functional object. 'mesh' should be a list of numpy arrays. [source] PDE sciann.constraints.pde.PDE(pde, sol=None, mesh_ids=None, name='pde') PDE class to impose to the system. Arguments pde (Functional): The Functional object that pde if formed on. sol (np.ndarray): Expected output to set the pde to. If not provided, will be set to zero . mesh_ids (np.ndarray): A 1D numpy arrays consists of node-ids to impose the condition. name (String): A str for name of the pde. Returns Raises ValueError : 'pde' should be a functional object. 'mesh' should be a list of numpy arrays. [source] Dirichlet sciann.constraints.dirichlet.Dirichlet(cond, sol=None, mesh_ids=None, name='dirichlet') Dirichlet class to impose to the system. Arguments cond (Functional): The Functional object that Dirichlet condition will be imposed on. sol (np.ndarray): Expected output to set the pde to. If not provided, will be set to zero . mesh_ids (np.ndarray): A 1D numpy arrays consists of node-ids to impose the condition. name (String): A str for name of the pde. Returns Raises ValueError : 'cond' should be a functional object. 'mesh' should be a list of numpy arrays. [source] Neumann sciann.constraints.neumann.Neumann(cond, sol=None, mesh_ids=None, var=None, name='neumann') Dirichlet class to impose to the system. Arguments cond (Functional): The Functional object that Neumann condition will be imposed on. sol (np.ndarray): Expected output to set the pde to. If not provided, will be set to zero . mesh_ids (np.ndarray): A 1D numpy arrays consists of node-ids to impose the condition. var (String): A layer name to differentiate cond with respect to. name (String): A str for name of the pde. Returns Raises ValueError : 'cond' should be a functional object. 'mesh' should be a list of numpy arrays. [source] Tie sciann.constraints.tie.Tie(cond1, cond2, sol=None, mesh_ids=None, name='tie') Tie class to constrain network outputs. constraint: cond1 - cond2 == sol . Arguments cond1 (Functional): A Functional object to be tied to cond2. cond2 (Functional): A 'Functional' object to be tied to cond1. sol (np.ndarray): Expected output to set the pde to. If not provided, will be set to zero . mesh_ids (np.ndarray): A 1D numpy arrays consists of node-ids to impose the condition. name (String): A str for name of the pde. Returns Raises ValueError : 'pde' should be a functional object. 'mesh' should be a list of numpy arrays.","title":"Constraints"},{"location":"constraints/#constraints","text":"[source]","title":"Constraints:"},{"location":"constraints/#data","text":"sciann.constraints.data.Data(cond, y_true=None, x_true_ids=None, name='data') Data class to impose to the system. Arguments cond (Functional): The Functional object that Data condition will be imposed on. y_true (np.ndarray): Expected output to set the pde to. If not provided, will be set to zero . x_true_ids (np.ndarray): A 1D numpy arrays consists of node-ids to impose the condition. name (String): A str for name of the pde. Returns Raises ValueError : 'cond' should be a functional object. 'mesh' should be a list of numpy arrays. [source]","title":"Data"},{"location":"constraints/#pde","text":"sciann.constraints.pde.PDE(pde, sol=None, mesh_ids=None, name='pde') PDE class to impose to the system. Arguments pde (Functional): The Functional object that pde if formed on. sol (np.ndarray): Expected output to set the pde to. If not provided, will be set to zero . mesh_ids (np.ndarray): A 1D numpy arrays consists of node-ids to impose the condition. name (String): A str for name of the pde. Returns Raises ValueError : 'pde' should be a functional object. 'mesh' should be a list of numpy arrays. [source]","title":"PDE"},{"location":"constraints/#dirichlet","text":"sciann.constraints.dirichlet.Dirichlet(cond, sol=None, mesh_ids=None, name='dirichlet') Dirichlet class to impose to the system. Arguments cond (Functional): The Functional object that Dirichlet condition will be imposed on. sol (np.ndarray): Expected output to set the pde to. If not provided, will be set to zero . mesh_ids (np.ndarray): A 1D numpy arrays consists of node-ids to impose the condition. name (String): A str for name of the pde. Returns Raises ValueError : 'cond' should be a functional object. 'mesh' should be a list of numpy arrays. [source]","title":"Dirichlet"},{"location":"constraints/#neumann","text":"sciann.constraints.neumann.Neumann(cond, sol=None, mesh_ids=None, var=None, name='neumann') Dirichlet class to impose to the system. Arguments cond (Functional): The Functional object that Neumann condition will be imposed on. sol (np.ndarray): Expected output to set the pde to. If not provided, will be set to zero . mesh_ids (np.ndarray): A 1D numpy arrays consists of node-ids to impose the condition. var (String): A layer name to differentiate cond with respect to. name (String): A str for name of the pde. Returns Raises ValueError : 'cond' should be a functional object. 'mesh' should be a list of numpy arrays. [source]","title":"Neumann"},{"location":"constraints/#tie","text":"sciann.constraints.tie.Tie(cond1, cond2, sol=None, mesh_ids=None, name='tie') Tie class to constrain network outputs. constraint: cond1 - cond2 == sol . Arguments cond1 (Functional): A Functional object to be tied to cond2. cond2 (Functional): A 'Functional' object to be tied to cond1. sol (np.ndarray): Expected output to set the pde to. If not provided, will be set to zero . mesh_ids (np.ndarray): A 1D numpy arrays consists of node-ids to impose the condition. name (String): A str for name of the pde. Returns Raises ValueError : 'pde' should be a functional object. 'mesh' should be a list of numpy arrays.","title":"Tie"},{"location":"contributing/","text":"On Github Issues and Pull Requests Found a bug? Have a new feature to suggest? Want to contribute changes to the codebase? Make sure to read this first. Adding new examples Even if you don't contribute to the Keras source code, if you have an application of Keras that is concise and powerful, please consider adding it to our collection of examples: Existing examples .","title":"On Github Issues and Pull Requests"},{"location":"contributing/#on-github-issues-and-pull-requests","text":"Found a bug? Have a new feature to suggest? Want to contribute changes to the codebase? Make sure to read this first.","title":"On Github Issues and Pull Requests"},{"location":"contributing/#adding-new-examples","text":"Even if you don't contribute to the Keras source code, if you have an application of Keras that is concise and powerful, please consider adding it to our collection of examples: Existing examples .","title":"Adding new examples"},{"location":"fields/","text":"Fields Field is a layer to define outputs of each Functional. It is very much similar to Keras' Dense layer. It is not necessary to be defined explicitly, however, if you are expecting multiple outputs, it is better to be defined using Field . from sciann import Field Fx = Field(10, name='Fx') [source] Field sciann.engine.functional.Field(units=1, name=None, activation=<function linear at 0x12763ec80>, kernel_initializer=<keras.initializers.VarianceScaling object at 0x1277112e8>, bias_initializer=<keras.initializers.RandomUniform object at 0x1277112b0>, trainable=True, dtype=None) Configures the Field class for the model outputs. Arguments units : Positive integer. Dimension of the output of the network. name : String. Assigns a layer name for the output. activation : Callable. A callable object for the activation. kernel_initializer : Initializer for the kernel. Defaulted to a normal distribution. bias_initializer : Initializer for the bias. Defaulted to a normal distribution. trainable : Boolean to activate parameters of the network. dtype : data-type of the network parameters, can be ('float16', 'float32', 'float64'). Raises","title":"Fields"},{"location":"fields/#fields","text":"Field is a layer to define outputs of each Functional. It is very much similar to Keras' Dense layer. It is not necessary to be defined explicitly, however, if you are expecting multiple outputs, it is better to be defined using Field . from sciann import Field Fx = Field(10, name='Fx') [source]","title":"Fields"},{"location":"fields/#field","text":"sciann.engine.functional.Field(units=1, name=None, activation=<function linear at 0x12763ec80>, kernel_initializer=<keras.initializers.VarianceScaling object at 0x1277112e8>, bias_initializer=<keras.initializers.RandomUniform object at 0x1277112b0>, trainable=True, dtype=None) Configures the Field class for the model outputs. Arguments units : Positive integer. Dimension of the output of the network. name : String. Assigns a layer name for the output. activation : Callable. A callable object for the activation. kernel_initializer : Initializer for the kernel. Defaulted to a normal distribution. bias_initializer : Initializer for the bias. Defaulted to a normal distribution. trainable : Boolean to activate parameters of the network. dtype : data-type of the network parameters, can be ('float16', 'float32', 'float64'). Raises","title":"Field"},{"location":"functionals/","text":"Functionals A combination of neural network layers form a Functional . Mathematically, a functional is a general mapping from space \\(X\\) into some output space \\(Y\\). Once the parameters of this transformation are found, this mapping is called a function . Functional s are needed to form SciModels . A Functional is a class to form complex architectures (mappings) from inputs ( Variables ) to the outputs. from sciann import Variable, Functional x = Variable('x') y = Variable('y') Fxy = Functional('Fxy', [x, y], hidden_layers=[10, 20, 10], activation='tanh') Functionals can be plotted when a SciModel is formed. A minimum of one Constraint is needed to form the SciModel from sciann.conditions import Data from sciann import SciModel model = SciModel(x, Data(Fxy), plot_to_file='output.png') [source] Functional sciann.engine.functional.Functional(fields=None, variables=None, hidden_layers=None, activation='linear', enrichment='linear', kernel_initializer=<keras.initializers.VarianceScaling object at 0x1277112e8>, bias_initializer=<keras.initializers.RandomUniform object at 0x1277112b0>, dtype=None, trainable=True) Configures the Functional object (Neural Network). Arguments fields (String or Field): [Sub-]Network outputs. It can be of type String - Associated fields will be created internally. It can be of type Field or Functional variables (Variable): [Sub-]Network inputs. It can be of type Variable or other Functional objects. - hidden_layers : A list indicating neurons in the hidden layers. e.g. [10, 100, 20] is a for hidden layers with 10, 100, 20, respectively. - activation : Activation function for the hidden layers. Last layer will have a linear output. - enrichment : Activation function to be applied to the network output. - kernel_initializer : Initializer of the Kernel , from k.initializers . - bias_initializer : Initializer of the Bias , from k.initializers . - dtype : data-type of the network parameters, can be ('float16', 'float32', 'float64'). Note: Only network inputs should be set. trainable (Boolean): False if network is not trainable, True otherwise. Default value is True. Raises ValueError : TypeError : [source] Variable sciann.engine.functional.Variable(name=None, tensor=None, dtype=None) Configures the Variable object for the network's input. Arguments name : String. Required as derivatives work only with layer names. tensor : Tensorflow Tensor . Can be pass as the input path. dtype : data-type of the network parameters, can be ('float16', 'float32', 'float64'). Raises [source] Field sciann.engine.functional.Field(units=1, name=None, activation=<function linear at 0x12763ec80>, kernel_initializer=<keras.initializers.VarianceScaling object at 0x1277112e8>, bias_initializer=<keras.initializers.RandomUniform object at 0x1277112b0>, trainable=True, dtype=None) Configures the Field class for the model outputs. Arguments units : Positive integer. Dimension of the output of the network. name : String. Assigns a layer name for the output. activation : Callable. A callable object for the activation. kernel_initializer : Initializer for the kernel. Defaulted to a normal distribution. bias_initializer : Initializer for the bias. Defaulted to a normal distribution. trainable : Boolean to activate parameters of the network. dtype : data-type of the network parameters, can be ('float16', 'float32', 'float64'). Raises","title":"Functionals"},{"location":"functionals/#functionals","text":"A combination of neural network layers form a Functional . Mathematically, a functional is a general mapping from space \\(X\\) into some output space \\(Y\\). Once the parameters of this transformation are found, this mapping is called a function . Functional s are needed to form SciModels . A Functional is a class to form complex architectures (mappings) from inputs ( Variables ) to the outputs. from sciann import Variable, Functional x = Variable('x') y = Variable('y') Fxy = Functional('Fxy', [x, y], hidden_layers=[10, 20, 10], activation='tanh') Functionals can be plotted when a SciModel is formed. A minimum of one Constraint is needed to form the SciModel from sciann.conditions import Data from sciann import SciModel model = SciModel(x, Data(Fxy), plot_to_file='output.png') [source]","title":"Functionals"},{"location":"functionals/#functional","text":"sciann.engine.functional.Functional(fields=None, variables=None, hidden_layers=None, activation='linear', enrichment='linear', kernel_initializer=<keras.initializers.VarianceScaling object at 0x1277112e8>, bias_initializer=<keras.initializers.RandomUniform object at 0x1277112b0>, dtype=None, trainable=True) Configures the Functional object (Neural Network). Arguments fields (String or Field): [Sub-]Network outputs. It can be of type String - Associated fields will be created internally. It can be of type Field or Functional variables (Variable): [Sub-]Network inputs. It can be of type Variable or other Functional objects. - hidden_layers : A list indicating neurons in the hidden layers. e.g. [10, 100, 20] is a for hidden layers with 10, 100, 20, respectively. - activation : Activation function for the hidden layers. Last layer will have a linear output. - enrichment : Activation function to be applied to the network output. - kernel_initializer : Initializer of the Kernel , from k.initializers . - bias_initializer : Initializer of the Bias , from k.initializers . - dtype : data-type of the network parameters, can be ('float16', 'float32', 'float64'). Note: Only network inputs should be set. trainable (Boolean): False if network is not trainable, True otherwise. Default value is True. Raises ValueError : TypeError : [source]","title":"Functional"},{"location":"functionals/#variable","text":"sciann.engine.functional.Variable(name=None, tensor=None, dtype=None) Configures the Variable object for the network's input. Arguments name : String. Required as derivatives work only with layer names. tensor : Tensorflow Tensor . Can be pass as the input path. dtype : data-type of the network parameters, can be ('float16', 'float32', 'float64'). Raises [source]","title":"Variable"},{"location":"functionals/#field","text":"sciann.engine.functional.Field(units=1, name=None, activation=<function linear at 0x12763ec80>, kernel_initializer=<keras.initializers.VarianceScaling object at 0x1277112e8>, bias_initializer=<keras.initializers.RandomUniform object at 0x1277112b0>, trainable=True, dtype=None) Configures the Field class for the model outputs. Arguments units : Positive integer. Dimension of the output of the network. name : String. Assigns a layer name for the output. activation : Callable. A callable object for the activation. kernel_initializer : Initializer for the kernel. Defaulted to a normal distribution. bias_initializer : Initializer for the bias. Defaulted to a normal distribution. trainable : Boolean to activate parameters of the network. dtype : data-type of the network parameters, can be ('float16', 'float32', 'float64'). Raises","title":"Field"},{"location":"scimodels/","text":"SciModels [source] SciModel sciann.engine.models.SciModel(inputs=None, constraints=None, loss_func='mse', plot_to_file=None) Configures the model for training. Arguments inputs : Main variables of the network, also known as xs , should be of type Variable . constraints : list all conditions to be imposed on the training; should be of type Constraint . plot_to_file : A string fine name to output the network architecture. Returns Raises ValueError : inputs must be of type Variable. constraints must be of type Functional. solve solve(x_true, weights=None, epochs=10, batch_size=256, shuffle=True, callbacks=None, stop_after=100, default_zero_weight=1e-10) Performs the training on the model. Arguments epochs : Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. batch_size : Integer or 'None'. Number of samples per gradient update. If unspecified, 'batch_size' will default to 128. shuffle : Boolean (whether to shuffle the training data). Default value is True. callbacks : List of keras.callbacks.Callback instances. Returns A 'History' object after performing fitting.","title":"SciModels"},{"location":"scimodels/#scimodels","text":"[source]","title":"SciModels"},{"location":"scimodels/#scimodel","text":"sciann.engine.models.SciModel(inputs=None, constraints=None, loss_func='mse', plot_to_file=None) Configures the model for training. Arguments inputs : Main variables of the network, also known as xs , should be of type Variable . constraints : list all conditions to be imposed on the training; should be of type Constraint . plot_to_file : A string fine name to output the network architecture. Returns Raises ValueError : inputs must be of type Variable. constraints must be of type Functional.","title":"SciModel"},{"location":"scimodels/#solve","text":"solve(x_true, weights=None, epochs=10, batch_size=256, shuffle=True, callbacks=None, stop_after=100, default_zero_weight=1e-10) Performs the training on the model. Arguments epochs : Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. batch_size : Integer or 'None'. Number of samples per gradient update. If unspecified, 'batch_size' will default to 128. shuffle : Boolean (whether to shuffle the training data). Default value is True. callbacks : List of keras.callbacks.Callback instances. Returns A 'History' object after performing fitting.","title":"solve"},{"location":"utils/","text":"Utils: gradients sciann.utils.gradients(xs, order=1) Returns the gradients of y in ys w.r.t. x in xs . ys and xs are each a Tensor or a list of tensors. Arguments ys : A tensor or list of tesnors to be differentiated. xs : A tensor or list of tensors to be used for differentiation. order : Order of differentiation. Returns A list of D^n y / Dx^n for each y and x in ys and xs . lambda_gradient sciann.utils.lambda_gradient(xs, order=1, name='') Returns the gradients of y in ys w.r.t. x in xs using Lambda layers. ys and xs are each a Tensor or a list of tensors. Arguments ys : A tensor or list of tesnors to be differentiated. xs : A tensor or list of tensors to be used for differentiation. name : A str name for the Lambda layer. Returns A tuple, (layers, grads) . - layers : A Lambda layer or list of Lambda layers where the gradient operator is applied. - grads : A gradient tensor or list of gradient tensors. diff sciann.utils.diff() Computes diff of functional object f. Arguments f : Functional object. ys : layer name for ys to differentiate. xs : layer name for xs to be differentiated w.r.t. order : order of differentiation w.r.t. xs - defaulted to 1. Returns A new functional object. radial_basis sciann.utils.radial_basis(ci, radii) Apply radial_basis function to x element-wise. Arguments xs : List of functional objects. ci : Center of basis functional (same length as xs). radii : standard deviation or radius from the center. Returns A new functional object. sin sciann.utils.sin() Computes sin of x element-wise. Arguments x : Functional object. Returns A new functional object. cos sciann.utils.cos() Computes cos of x element-wise. Arguments x : Functional object. Returns A new functional object. tan sciann.utils.tan() Computes tan of x element-wise. Arguments x : Functional object. Returns A new functional object. tanh sciann.utils.tanh() Computes tanh of x element-wise. Arguments x : Functional object. Returns A new functional object. exp sciann.utils.exp() Computes exp of x element-wise. Arguments x : Functional object. Returns A new functional object. pow sciann.utils.pow(a) Element-wise exponentiation applied to the Functional object. Arguments f : Functional object. a : Python integer. Returns A Functional. add sciann.utils.add(other) Element-wise addition applied to the Functional objects. Arguments f : Functional object. other : A python number or a tensor or a functional object. Returns A Functional. sub sciann.utils.sub(other) Element-wise subtraction applied to the Functional objects. Arguments f : Functional object. other : A python number or a tensor or a functional object. Returns A Functional. mul sciann.utils.mul(other) Element-wise multiplication applied to the Functional objects. Arguments f : Functional object. other : A python number or a tensor or a functional object. Returns A Functional. div sciann.utils.div(other) Element-wise division applied to the Functional objects. Arguments f : Functional object. other : A python number or a tensor or a functional object. Returns A Functional.","title":"Utils"},{"location":"utils/#utils","text":"","title":"Utils:"},{"location":"utils/#gradients","text":"sciann.utils.gradients(xs, order=1) Returns the gradients of y in ys w.r.t. x in xs . ys and xs are each a Tensor or a list of tensors. Arguments ys : A tensor or list of tesnors to be differentiated. xs : A tensor or list of tensors to be used for differentiation. order : Order of differentiation. Returns A list of D^n y / Dx^n for each y and x in ys and xs .","title":"gradients"},{"location":"utils/#lambda_gradient","text":"sciann.utils.lambda_gradient(xs, order=1, name='') Returns the gradients of y in ys w.r.t. x in xs using Lambda layers. ys and xs are each a Tensor or a list of tensors. Arguments ys : A tensor or list of tesnors to be differentiated. xs : A tensor or list of tensors to be used for differentiation. name : A str name for the Lambda layer. Returns A tuple, (layers, grads) . - layers : A Lambda layer or list of Lambda layers where the gradient operator is applied. - grads : A gradient tensor or list of gradient tensors.","title":"lambda_gradient"},{"location":"utils/#diff","text":"sciann.utils.diff() Computes diff of functional object f. Arguments f : Functional object. ys : layer name for ys to differentiate. xs : layer name for xs to be differentiated w.r.t. order : order of differentiation w.r.t. xs - defaulted to 1. Returns A new functional object.","title":"diff"},{"location":"utils/#radial_basis","text":"sciann.utils.radial_basis(ci, radii) Apply radial_basis function to x element-wise. Arguments xs : List of functional objects. ci : Center of basis functional (same length as xs). radii : standard deviation or radius from the center. Returns A new functional object.","title":"radial_basis"},{"location":"utils/#sin","text":"sciann.utils.sin() Computes sin of x element-wise. Arguments x : Functional object. Returns A new functional object.","title":"sin"},{"location":"utils/#cos","text":"sciann.utils.cos() Computes cos of x element-wise. Arguments x : Functional object. Returns A new functional object.","title":"cos"},{"location":"utils/#tan","text":"sciann.utils.tan() Computes tan of x element-wise. Arguments x : Functional object. Returns A new functional object.","title":"tan"},{"location":"utils/#tanh","text":"sciann.utils.tanh() Computes tanh of x element-wise. Arguments x : Functional object. Returns A new functional object.","title":"tanh"},{"location":"utils/#exp","text":"sciann.utils.exp() Computes exp of x element-wise. Arguments x : Functional object. Returns A new functional object.","title":"exp"},{"location":"utils/#pow","text":"sciann.utils.pow(a) Element-wise exponentiation applied to the Functional object. Arguments f : Functional object. a : Python integer. Returns A Functional.","title":"pow"},{"location":"utils/#add","text":"sciann.utils.add(other) Element-wise addition applied to the Functional objects. Arguments f : Functional object. other : A python number or a tensor or a functional object. Returns A Functional.","title":"add"},{"location":"utils/#sub","text":"sciann.utils.sub(other) Element-wise subtraction applied to the Functional objects. Arguments f : Functional object. other : A python number or a tensor or a functional object. Returns A Functional.","title":"sub"},{"location":"utils/#mul","text":"sciann.utils.mul(other) Element-wise multiplication applied to the Functional objects. Arguments f : Functional object. other : A python number or a tensor or a functional object. Returns A Functional.","title":"mul"},{"location":"utils/#div","text":"sciann.utils.div(other) Element-wise division applied to the Functional objects. Arguments f : Functional object. other : A python number or a tensor or a functional object. Returns A Functional.","title":"div"},{"location":"variables/","text":"Variables Variable is a way to to define inputs to the network, very much similar to the Input class in Keras . However, since we need to perform differentiation and other operations on the network, we cannot just use Input . Instead, we need to define the inputs of the network through Variable . For scientific computations, a Variable has only a dimension of 1. Therefore, if you need to have a three-dimensional coordinate inputs, you need to define three variables: from sciann import Variable x = Variable('x') y = Variable('y') z = Variable('z') This is precisely because we need to perform differentiation with respect to (x, y, z). [source] Variable sciann.engine.functional.Variable(name=None, tensor=None, dtype=None) Configures the Variable object for the network's input. Arguments name : String. Required as derivatives work only with layer names. tensor : Tensorflow Tensor . Can be pass as the input path. dtype : data-type of the network parameters, can be ('float16', 'float32', 'float64'). Raises","title":"Variables"},{"location":"variables/#variables","text":"Variable is a way to to define inputs to the network, very much similar to the Input class in Keras . However, since we need to perform differentiation and other operations on the network, we cannot just use Input . Instead, we need to define the inputs of the network through Variable . For scientific computations, a Variable has only a dimension of 1. Therefore, if you need to have a three-dimensional coordinate inputs, you need to define three variables: from sciann import Variable x = Variable('x') y = Variable('y') z = Variable('z') This is precisely because we need to perform differentiation with respect to (x, y, z). [source]","title":"Variables"},{"location":"variables/#variable","text":"sciann.engine.functional.Variable(name=None, tensor=None, dtype=None) Configures the Variable object for the network's input. Arguments name : String. Required as derivatives work only with layer names. tensor : Tensorflow Tensor . Can be pass as the input path. dtype : data-type of the network parameters, can be ('float16', 'float32', 'float64'). Raises","title":"Variable"},{"location":"why-use-sciann/","text":"Why use SciANN among all other codes? The main purpose of SciANN is a platform for people with Scientific Computations backgrounds in mind. You will find this code very useful for: Solving ODEs and PDEs using densely connect, complex networks, recurrent networks are on the way. This platform is ready to use for Curve Fitting, Differentiations, Integration, etc. If you have other scientific computations in mind that are not implemented yet, contact us . As an example, let's fit a neural network with three-hidden layers, each with 10 neurons and \\( \\tanh \\) activation function, on data generated from \\( sin(x) \\): import numpy as np from sciann import Variable, Functional, SciModel from sciann.conditions import Data # Synthetic data generated from sin function over [0, 2pi] x_true = np.linspace(0, np.pi*2, 10000) y_true = np.sin(x_true) # The network inputs should be defined with Variable. x = Variable('x', dtype='float32') # Each network is defined by Functional. y = Functional('y', x, [10, 10, 10], activation='tanh') # The training data is a condition (constraint) on the model. c1 = Data(y, y_true) # The model is formed with input `x` and condition `c1`. model = SciModel(x, c1) # Training: .solve runs the optimization and finds the parameters. model.solve(x_true, batch_size=32, epochs=100) # used to evaluate the model after the training. y_pred = model.predict(x_true) As you may find, this code takes advantage of Keras great design and takes it to the next level for scientific computations. w","title":"Why use SciANN"},{"location":"why-use-sciann/#why-use-sciann-among-all-other-codes","text":"The main purpose of SciANN is a platform for people with Scientific Computations backgrounds in mind. You will find this code very useful for: Solving ODEs and PDEs using densely connect, complex networks, recurrent networks are on the way. This platform is ready to use for Curve Fitting, Differentiations, Integration, etc. If you have other scientific computations in mind that are not implemented yet, contact us . As an example, let's fit a neural network with three-hidden layers, each with 10 neurons and \\( \\tanh \\) activation function, on data generated from \\( sin(x) \\): import numpy as np from sciann import Variable, Functional, SciModel from sciann.conditions import Data # Synthetic data generated from sin function over [0, 2pi] x_true = np.linspace(0, np.pi*2, 10000) y_true = np.sin(x_true) # The network inputs should be defined with Variable. x = Variable('x', dtype='float32') # Each network is defined by Functional. y = Functional('y', x, [10, 10, 10], activation='tanh') # The training data is a condition (constraint) on the model. c1 = Data(y, y_true) # The model is formed with input `x` and condition `c1`. model = SciModel(x, c1) # Training: .solve runs the optimization and finds the parameters. model.solve(x_true, batch_size=32, epochs=100) # used to evaluate the model after the training. y_pred = model.predict(x_true) As you may find, this code takes advantage of Keras great design and takes it to the next level for scientific computations. w","title":"Why use SciANN among all other codes?"},{"location":"examples/curve-fitting-1d/","text":"Curve fitting in 1D {{autogenerated}}","title":"Curve fitting in 1D"},{"location":"examples/curve-fitting-1d/#curve-fitting-in-1d","text":"{{autogenerated}}","title":"Curve fitting in 1D"},{"location":"examples/example-fitting-1d/","text":"Curve fitting in 1D Here, a 1D curve fitting example is explored. Imagine, a synthetic data generated from \\( \\sin(x) \\) over the range of \\( [0, 2\\pi] \\). To train a neural network model on this curve, you should first define a Variable . A neural network with three layers, each containing 10 neurons, and with tanh activation function is then generated using the Functional class. The target is imposed on the output using the Data class from Constraint , and passed to the SciModel to form a Sciann model. import numpy as np from sciann import Variable, Functional, SciModel from sciann.constraints import Data # Synthetic data generated from sin function over [0, 2pi] x_true = np.linspace(0, np.pi*2, 10000) y_true = np.sin(x_true) # The network inputs should be defined with Variable. x = Variable('x', dtype='float32') # Each network is defined by Functional. y = Functional('y', x, [10, 10, 10], activation='tanh') # The training data is a condition (constraint) on the model. c1 = Data(y, y_true) # The model is formed with input `x` and condition `c1`. model = SciModel(x, c1) # Training: .solve runs the optimization and finds the parameters. model.solve(x_true, batch_size=32, epochs=100) # used to evaluate the model after the training. y_pred = model.predict(x_true)","title":"Curve fitting in 1D"},{"location":"examples/example-fitting-1d/#curve-fitting-in-1d","text":"Here, a 1D curve fitting example is explored. Imagine, a synthetic data generated from \\( \\sin(x) \\) over the range of \\( [0, 2\\pi] \\). To train a neural network model on this curve, you should first define a Variable . A neural network with three layers, each containing 10 neurons, and with tanh activation function is then generated using the Functional class. The target is imposed on the output using the Data class from Constraint , and passed to the SciModel to form a Sciann model. import numpy as np from sciann import Variable, Functional, SciModel from sciann.constraints import Data # Synthetic data generated from sin function over [0, 2pi] x_true = np.linspace(0, np.pi*2, 10000) y_true = np.sin(x_true) # The network inputs should be defined with Variable. x = Variable('x', dtype='float32') # Each network is defined by Functional. y = Functional('y', x, [10, 10, 10], activation='tanh') # The training data is a condition (constraint) on the model. c1 = Data(y, y_true) # The model is formed with input `x` and condition `c1`. model = SciModel(x, c1) # Training: .solve runs the optimization and finds the parameters. model.solve(x_true, batch_size=32, epochs=100) # used to evaluate the model after the training. y_pred = model.predict(x_true)","title":"Curve fitting in 1D"},{"location":"getting-started/functional-guide/","text":"Using Functional to form complex network architectures The Functional class is designed to allow users to design complex networks with a few lines of code. To use Functional, you can follow the exmaple bellow: import numpy as np from sciann Variable, Functional, SciModel from sciann.conditions Data # Synthetic data to be fitted. x_true = np.linspace(0.0, 2*np.pi, 10000) y_true = np.sin(x_true) # Functional requires input features to be defined through Variable. x = Variable(\"x\", dtype='float32') # A complex network with 5 hidden layers ([5, 10, 20, 10, 5]), # and feature aumentation [x, x**2, x**3, sin(x), cos(x), sinh(x)]. y = Functional( \"y\", [x, x**2, x**3, sin(x), cos(x), sinh(x)], hidden_layers = [5, 10, 20, 10, 5], activations = 'tanh', ) # Define the SciModel. model = SciModel(x, Data(y, y_true)) # Solve the neural network model. model.solve(x_true, epochs=32, batches=10) # Find model's prediciton. x_pred = model.predict(x_true)","title":"Guide to Functional"},{"location":"getting-started/functional-guide/#using-functional-to-form-complex-network-architectures","text":"The Functional class is designed to allow users to design complex networks with a few lines of code. To use Functional, you can follow the exmaple bellow: import numpy as np from sciann Variable, Functional, SciModel from sciann.conditions Data # Synthetic data to be fitted. x_true = np.linspace(0.0, 2*np.pi, 10000) y_true = np.sin(x_true) # Functional requires input features to be defined through Variable. x = Variable(\"x\", dtype='float32') # A complex network with 5 hidden layers ([5, 10, 20, 10, 5]), # and feature aumentation [x, x**2, x**3, sin(x), cos(x), sinh(x)]. y = Functional( \"y\", [x, x**2, x**3, sin(x), cos(x), sinh(x)], hidden_layers = [5, 10, 20, 10, 5], activations = 'tanh', ) # Define the SciModel. model = SciModel(x, Data(y, y_true)) # Solve the neural network model. model.solve(x_true, epochs=32, batches=10) # Find model's prediciton. x_pred = model.predict(x_true)","title":"Using Functional to form complex network architectures"},{"location":"getting-started/scimodel-guide/","text":"Getting started with the SciANN model or SciModel The SciModel is the relation between network inputs, i.e. Variable and network outputs, i.e. Conditions . You can set up a SciModel as simple as the code bellow: from sciann import Variable, Functional, SciModel from sciann.conditions import Data x = Variable(\"x\") y = Functional(\"y\", x) cy = Data(y) model = SciModel(cy)","title":"Guide to SciANN model"},{"location":"getting-started/scimodel-guide/#getting-started-with-the-sciann-model-or-scimodel","text":"The SciModel is the relation between network inputs, i.e. Variable and network outputs, i.e. Conditions . You can set up a SciModel as simple as the code bellow: from sciann import Variable, Functional, SciModel from sciann.conditions import Data x = Variable(\"x\") y = Functional(\"y\", x) cy = Data(y) model = SciModel(cy)","title":"Getting started with the SciANN model or SciModel"}]}